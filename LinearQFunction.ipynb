{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearQFunction():\n",
    "    def __init__(self, n_features, action_space = 4, weights=None, default=0.0, lr = 0.0001, gamma = 0.9, epsilon = 0.91, annealing_coefficient = 0.999999):\n",
    "        #In this case, features represents the states the agent see\n",
    "        #n_features should be the number of states that the agent sees\n",
    "        #action_space should be the number of actions the agent can take\n",
    "        self.rng = np.random.default_rng(0)\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.action_space = action_space\n",
    "        self.lr = lr #learning rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.annealing_coefficient = annealing_coefficient\n",
    "\n",
    "        if weights == None:\n",
    "            self.weights = np.array(\n",
    "                [\n",
    "                    [default] * self.action_space\n",
    "                    for _ in range(0, n_features)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def set_RDG_seed(self, seed):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def update(self, curent_stacked_feature, action, next_stacked_features, reward, done, possible_next_actions):\n",
    "        # update the weights\n",
    "        q_next = np.zeros(len(possible_next_actions))\n",
    "        q = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            q += self.weights[i][action] * curent_stacked_feature[i]\n",
    "\n",
    "            for j in range(len(possible_next_actions)):\n",
    "                q_next[j] += self.weights[i][possible_next_actions[j]] * next_stacked_features[i]\n",
    "\n",
    "        q_next = np.max(q_next)\n",
    "        #for stack in self.weights:\n",
    "        td_error = reward + self.gamma * q_next * (1-done) - q\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i][action] += self.lr * td_error * curent_stacked_feature[i]\n",
    "\n",
    "        #annealing epsilon\n",
    "        if self.epsilon > 0.1:\n",
    "            self.epsilon *= self.annealing_coefficient\n",
    "    \n",
    "    def take_action(self, possible_actions, curent_stacked_feature):\n",
    "        if(self.rng.random() < self.epsilon):\n",
    "            return self.rng.choice(possible_actions)\n",
    "        else:\n",
    "            q = np.zeros(len(possible_actions))\n",
    "     \n",
    "            for i in range(len(curent_stacked_feature)):\n",
    "                for j in range( len(possible_actions) ):\n",
    "                    q[j] += self.weights[i][ possible_actions[j] ] * curent_stacked_feature[i]\n",
    "\n",
    "            arg_max_index = np.argmax(q)\n",
    "            return possible_actions[arg_max_index]\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
